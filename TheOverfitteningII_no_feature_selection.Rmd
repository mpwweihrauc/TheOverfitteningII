---
title: "TheOverfitteningII"
author: "Martin Weihrauch"
date: "`r format(sys.time(), %d %B %Y)`"
output: 
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
---

```{r Loading_libraries, echo = FALSE, warning = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", dependencies = TRUE)
if(!require(caret)) install.packages("caret")
if(!require(doParallel)) install.packages("doParallel")
if(!require(vtreat)) install.packages("vtreat")
if(!require(magrittr)) install.packages("magrittr")
if(!require(vip)) install.packages("vip")
if(!require(glmnet)) install.packages("glmnet")
```

```{r Dataset_read_in, echo = FALSE, warning = FALSE, message = FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")

train_caret <- train
train_caret$target <- as.factor(make.names(train$target))
train_caret <- subset(train_caret, select = -id)

test_caret <- test
test_caret <- subset(test_caret, select = -id)
```


```{r glmnet_model}
# Paralellize.
cluster <- makeCluster(detectCores(logical = FALSE), type = "PSOCK")
registerDoParallel(cluster)

# Train control.
# Alpha 0 = ridge regression L2 penalty; alpha 1 = lasso regression L1 penalty (penalizes sum of absolute values)
# Lasso regression works well with many weak predictors, penalizing most of them to 0 contribution.
# Ridge regression works well with several strong predictors.
tuneGrid <- expand.grid(
                        .alpha = seq(0.05, 1, 0.05),
                        .lambda = seq(0.2, 1, 0.01)
                        )

trControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          summaryFunction = twoClassSummary,
                          returnResamp = "all",
                          classProbs = TRUE
                          )

# Train an "xgbTree" model with caret.
glmnet_fit <- caret::train(data = train_caret,
                    target ~ .,
                    method = "glmnet",
                    family = "binomial",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    preProcess = c("center", "scale"))

# Back to sequential.
stopCluster(cluster)

# Analyse glmnet_fitting results.
glmnet_fit$bestTune

# Visualization of the most important features.
vip(glmnet_fit, num_features = 10) + ggtitle("Variable importance")

# Plot tuning.
ggplot(glmnet_fit)

# Predicting on test_set.
glmnet_fit_pred <- predict(glmnet_fit, test_caret, type = "prob")

# We create the submission file.
glmnet_pred <- data.frame(id = test$id, target = glmnet_fit_pred$X1)

colnames(glmnet_pred) <- c("id", "target")

write.table(glmnet_pred, file = "glmnet_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```


```{r ranger_model_prediction}
# Paralellize.
cluster <- makeCluster(detectCores(logical = FALSE), type = "PSOCK")
registerDoParallel(cluster)

# Create a tuning grid for testing hyperparameters. Optimal mtry for ranger was around 42.
tuneGrid <- expand.grid(
                        .mtry = c(60, 160),
                        .splitrule = "extratrees",
                        .min.node.size = 10
                       )
# Train control.
trControl <- trainControl(method = "repeatedcv",
                          number = 3,
                          repeats = 1,
                          returnResamp = "all",
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary
                          )


# Train ranger model with caret.
ranger_fit <- caret::train(
                    data = train_caret,
                    target ~ .,
                    method = "ranger",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    preProcess = c("center", "scale"),
                    num.trees = 500,
                    importance = "permutation")

# Back to sequential.
stopCluster(cluster)

# Analyse ranger_fitting results.
ranger_fit$bestTune

# Visualization of the most important features.
vip(ranger_fit, num_features = 10) + ggtitle("Variable importance")

# Plot tuning.
ggplot(ranger_fit)

# Predicting on test_set.
ranger_fit_pred <- predict(ranger_fit, test_caret, type = "prob")

# We create the submission file.
ranger_pred <- data.frame(id = test$id, target = ranger_fit_pred$X1)

colnames(ranger_pred) <- c("id", "target")

write.table(ranger_pred, file = "ranger_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```


```{r}
# Create a tuning grid for testing hyperparameters.
tuneGrid <- expand.grid(
                  nrounds = seq(25, 4000, 25),
                   eta = 0.001,
                   max_depth = 2,
                   gamma = 0.4,
                   colsample_bytree = 0.6,
                   min_child_weight = 1,
                   subsample = 0.2
                    )
# Train control.
trControl <- trainControl(
                          method = "repeatedcv",
                          number = 10,
                          repeats = 3,
                          returnResamp = "all",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

cluster <- makeCluster(detectCores(logical = FALSE), type = "PSOCK")
registerDoParallel(cluster)

# Train an "xgbTree" model with caret.
xgbtree_fit <- caret::train(data = train_caret,
                    target ~ .,
                    method = "xgbTree",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    nthread = 1)

# Back to sequential.
stopCluster(cluster)

# Analyse xgbtree_fitting results.
xgbtree_fit$bestTune

# Visualization of the most important features.
vip(xgbtree_fit, num_features = 10) + ggtitle("Variable importance")

# Plot tuning.
ggplot(xgbtree_fit)

# Predicting on test_set.
xgbtree_fit_pred <- predict(xgbtree_fit, test_caret, type = "prob")

# We create the submission file.
xgbtree_pred <- data.frame(id = test$id, target = xgbtree_fit_pred$X1)

colnames(xgbtree_pred) <- c("id", "target")

write.table(xgbtree_pred, file = "xgbtree_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")

```


```{r}
# We create the submission file.
combined_pred <- data.frame(id = test$id, target = (ranger_fit_pred$X1 + glmnet_fit_pred$X1 + xgbtree_fit_pred$X1) / 3)

colnames(combined_pred) <- c("id", "target")

write.table(combined_pred, file = "combined_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```
