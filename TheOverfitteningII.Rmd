---
title: "TheOverfitteningII"
author: "Martin Weihrauch"
date: "`r format(sys.time(), %d %B %Y)`"
output: 
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
---

```{r Loading_libraries, echo = FALSE, warning = FALSE}
if(!require(tidyverse)) install.packages("tidyverse", dependencies = TRUE)
if(!require(caret)) install.packages("caret")
if(!require(doParallel)) install.packages("doParallel")
if(!require(vtreat)) install.packages("vtreat")
if(!require(magrittr)) install.packages("magrittr")
if(!require(vip)) install.packages("vip")
if(!require(glmnet)) install.packages("glmnet")
```

```{r Dataset_read_in, echo = FALSE, warning = FALSE, message = FALSE}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


```{r}
# Create a tuning grid for testing hyperparameters.
tuneGrid <- expand.grid(nrounds = seq(1, 100, 1),
                   eta = 0.001,
                   max_depth = 2,
                   gamma = 10,
                   colsample_bytree = 0.7,
                   min_child_weight = 1,
                   subsample = 0.7
                    )
# Train control.
trControl <- trainControl(method = "cv",
                          number = 10,
                          returnResamp = "all",
                          summaryFunction = twoClassSummary,
                          classProbs = TRUE)

# Paralellize.
cluster <- makeCluster(detectCores(logical = TRUE))
registerDoParallel(cluster)

# Train an "xgbTree" model with caret.
fit <- caret::train(data = subset(train, select = -c(id)),
                    target ~ .,
                    method = "xgbTree",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    nthread = 1)

# Back to sequential.
stopCluster(cluster)
registerDoSEQ()

# Analyse fitting results.
fit$bestTune

# Visualization of the most important features.
vip(fit, num_features = 10) + ggtitle("Variable importance")

# Plot tuning.
ggplot(fit)

```

```{r, echo = FALSE}
# Predicting on test_set.
fit_pred <- predict(fit, test, type = "prob") %>% as.character() %>% parse_number()

# We create the submission file.
my_submission <- data.frame(id = test$id, target = fit_pred)

write.table(my_submission, file = "submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            sep = ",")
```


```{r ranger_model}
# Paralellize.
cluster <- makeCluster(detectCores(logical = TRUE))
registerDoParallel(cluster)

train_caret <- train
train_caret$target <- as.factor(make.names(train$target))

# Create a tuning grid for testing hyperparameters. Optimal mtry for ranger was around 42.
tuneGrid <- expand.grid(
                        .mtry = seq(40, 60, 1),
                        .splitrule = "gini",
                        .min.node.size = 10
                       )
# Train control.
trControl <- trainControl(method = "repeatedcv",
                          number = 5,
                          repeats = 5,
                          returnResamp = "all",
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary
                          )



# Train ranger model with caret.
fit <- caret::train(
                    data = train_caret %>% dplyr::select(-id),
                    target ~ .,
                    method = "ranger",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    preProcess = c("center", "scale")
                    num.trees = 500,
                    importance = "permutation")

# Back to sequential.
stopCluster(cluster)
registerDoSEQ()

# Analyse fitting results.
fit$bestTune

# Visualization of the most important features.
vip(fit, num_features = 15) + ggtitle("Variable importance")
ranger_select <- vip(fit, num_features = 15)
ranger_select <- ranger_select$data$Variable

# Plot tuning.
ggplot(fit)

# Predicting on test_set.
fit_pred <- predict(fit, test, type = "prob")

# We create the submission file.
my_submission <- data.frame(id = test$id, target = fit_pred$X1)

colnames(my_submission) <- c("id", "target")

write.table(my_submission, file = "submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")

```


```{r recursive_feature_elimination}
train_caret <- train
train_caret$target <- as.factor(make.names(train$target))

# Paralellize.
cluster <- makeCluster(detectCores(logical = TRUE))
registerDoParallel(cluster)


caretFuncs$summary <- twoClassSummary


rfeControl <- rfeControl(
        functions = caretFuncs, 
        method = "cv",
        number = 5,
        saveDetails = TRUE,
        returnResamp = "all"
        )

trControl <- trainControl(
        classProbs= TRUE,
        summaryFunction = twoClassSummary)

rfe_model <- rfe(
    y = train_caret %>% dplyr::pull(target), 
    x = train_caret %>% dplyr::select(-target, -id),  
    sizes = c(1:300), 
    rfeControl = rfeControl, 
    trControl = trControl, 
    method = "glmnet", 
    metric = "ROC")

# Back to sequential.
stopCluster(cluster)
registerDoSEQ()


print(rfe_model)

print(rfe_model$results)

print(rfe_model$optVariables)

rfe_select <- rfe_model$optVariables
```

```{r glmnet_model}
# Paralellize.
cluster <- makeCluster(detectCores(logical = TRUE))
registerDoParallel(cluster)


train_caret <- train
train_caret$target <- as.factor(make.names(train$target))
train_caret <- subset(train_caret, select = -id)
  
# Use the most important variables from all predictions.
selection <- unique(rfe_select, ranger_select)
  
# Train control.
# Alpha 0 = ridge regression L2 penalty; alpha 1 = lasso regression L1 penalty (penalizes sum of absolute values)
# Lasso regression works well with many weak predictors, penalizing most of them to 0 contribution.
# Ridge regression works well with several strong predictors.
tuneGrid <- expand.grid(
                        .alpha = 0,
                        .lambda = seq(0, 1, length = 100)
                        )

trControl <- trainControl(method = "repeatedcv",
                          number = 5,
                          repeats = 5,
                          summaryFunction = twoClassSummary,
                          returnResamp = "all",
                          classProbs = TRUE
                          )

# Train an "xgbTree" model with caret.
glmnet_fit <- caret::train(data = train_caret %>% dplyr::select(target, selection),
                    target ~ .,
                    method = "glmnet",
                    family = "binomial",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    preProcess = c("center", "scale"))

# Back to sequential.
stopCluster(cluster)
registerDoSEQ()


# Analyse glmnet_fitting results.
glmnet_fit$bestTune

# Visualization of the most important features.
vip(glmnet_fit, num_features = 15) + ggtitle("Variable importance")
# p <- vip(glmnet_fit, num_features = 30)
# selection <- p$data$Variable

# Plot tuning.
ggplot(glmnet_fit)

# Predicting on test_set.
glmnet_fit_pred <- predict(glmnet_fit, subset(test, select = selection), type = "prob")

# We create the submission file.
glmnet_pred <- data.frame(id = test$id, target = glmnet_fit_pred$X1)

colnames(glmnet_pred) <- c("id", "target")

write.table(glmnet_pred, file = "glmnet_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```

```{r ranger_model_prediction}
# Paralellize.
cluster <- makeCluster(detectCores(logical = TRUE))
registerDoParallel(cluster)

train_caret <- train
train_caret$target <- as.factor(make.names(train$target))

# Create a tuning grid for testing hyperparameters. Optimal mtry for ranger was around 42.
tuneGrid <- expand.grid(
                        .mtry = seq(1, 15, 1),
                        .splitrule = "gini",
                        .min.node.size = 10
                       )
# Train control.
trControl <- trainControl(method = "repeatedcv",
                          number = 5,
                          repeats = 5,
                          returnResamp = "all",
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary
                          )



# Train ranger model with caret.
ranger_fit <- caret::train(
                    data = train_caret %>% dplyr::select(target, selection),
                    target ~ .,
                    method = "ranger",
                    metric = "ROC",
                    trControl = trControl,
                    tuneGrid = tuneGrid,
                    preProcess = c("center", "scale"),
                    num.trees = 1200,
                    importance = "permutation")

# Back to sequential.
stopCluster(cluster)
registerDoSEQ()

# Analyse ranger_fitting results.
ranger_fit$bestTune

# Visualization of the most important features.
vip(ranger_fit, num_features = 15) + ggtitle("Variable importance")

# Plot tuning.
ggplot(ranger_fit)

# Predicting on test_set.
ranger_fit_pred <- predict(ranger_fit, test, type = "prob")

# We create the submission file.
ranger_pred <- data.frame(id = test$id, target = ranger_fit_pred$X1)

colnames(ranger_pred) <- c("id", "target")

write.table(ranger_pred, file = "ranger_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```


```{r}
# We create the submission file.
combined_pred <- data.frame(id = test$id, target = (ranger_fit_pred$X1 + glmnet_fit_pred$X1) / 2)

colnames(combined_pred) <- c("id", "target")

write.table(combined_pred, file = "combined_submission.csv",
            col.names = TRUE,
            row.names = FALSE,
            quote = FALSE,
            sep = ",")
```
